{
    "requirement": "清新 AI 问答小助手来啦～ 每个助手提供身份定位和问题引导，底部轻松输入提问（支持文本 / 语音）～ 还能创建专属 AI 智能体（雅思教练、诗词达人等），调整回答长度（0-500 字）和风格（温度参数），聊天超有趣～ 后端需提供稳定支撑：含 AI 模型 API 集成服务（支持流式输出）、对话上下文存储（Redis 缓存）、智能体数据管理（MySQL 存储参数 / 开场白）、格式解析服务（markdown/latex 转 HTML），确保实时交互与数据安全。",
    "dialogue": "实现基础功能：前端完成清新界面搭建（简洁对话框、浅灰圆角常见问题提示）、实时对话（输入 / 语音上传、流式响应展示）、参数调节（长度 / 温度进度条）；后端配套 API 网关（请求转发 / 频率限制）、AI 模型调用服务（流式输出处理）、上下文管理（对话历史缓存），保障对话流畅。升级体验：前端新增智能体管理（侧边栏列表、新建 / 编辑弹窗）、格式渲染（markdown/latex 解析）、移动端适配（左上显侧边栏、加号进设置）；后端开发智能体数据 CRUD 接口（保存参数 / 开场白）、角色重设接口（清空上下文）、语音识别 API 集成（支持语音转文字），提升个性化与兼容性。",
    "task": "清新 AI 问答小助手来啦～ 每个助手提供身份定位和问题引导，底部轻松输入提问（支持文本 / 语音）～ 还能创建专属 AI 智能体（雅思教练、诗词达人等），调整回答长度（0-500 字）和风格（温度参数），聊天超有趣～ 后端需提供稳定支撑：含 AI 模型 API 集成服务（支持流式输出）、对话上下文存储（Redis 缓存）、智能体数据管理（MySQL 存储参数 / 开场白）、格式解析服务（markdown/latex 转 HTML）；实现基础功能：前端完成清新界面搭建（简洁对话框、浅灰圆角常见问题提示）、实时对话（输入 / 语音上传、流式响应展示）、参数调节（长度 / 温度进度条）；后端配套 API 网关（请求转发 / 频率限制）、AI 模型调用服务（流式输出处理）、上下文管理（对话历史缓存）；升级体验：前端新增智能体管理（侧边栏列表、新建 / 编辑弹窗）、格式渲染（markdown/latex 解析）、移动端适配（左上显侧边栏、加号进设置）；后端开发智能体数据 CRUD 接口（保存参数 / 开场白）、角色重设接口（清空上下文）、语音识别 API 集成（支持语音转文字），确保操作便捷且交互有趣。",
    "refined_task": " 一、需求描述 开发一个智能 AI 问答助手网页，解决用户日常问答、创作和娱乐需求。用户可通过选择不同智能体（如通用助手、雅思教练）或自定义参数（答案长度 0-500 字、温度），获得个性化 AI 对话体验（问答、创作、互动）。网页需提供清新界面、实时流式聊天、参数调节、格式渲染功能；后端需搭建 AI 模型集成、数据存储、接口服务体系，确保操作便捷、交互流畅且数据稳定。 二、详细功能 1. 基础功能 - 前端： - 身份定位与引导：每个智能体展示专属身份（如 “通用问答助手”），底部左下角用浅灰圆角框包裹 3 条常见问题引导（如 “如何调整回答长度？”）； - 实时对话：底部输入框支持文本输入（提示 “输入问题或按住说话...”）和语音输入（按住说话转文字），点击发送后调用后端 API，流式展示 AI 响应（逐字加载效果），保留对话上下文（显示历史消息）； - 参数调节：对话框内添加 “答案长度”（0-500 字，进度条两端标 “0”“500”）和 “答案温度” 进度条，拖动实时更新参数值（如 “答案长度：200 字”），参数随请求传给后端。 - 后端： - API 网关服务：接收前端请求，做参数校验（如长度 / 温度范围）、请求频率限制（单用户每分钟≤30 次），转发至对应业务服务； - AI 模型集成服务：配置第三方 AI 模型 API（如 GPT、文心一言），设置 stream=true 实现流式输出，处理模型返回的 chunk 数据并推送给前端； - 上下文管理服务：用 Redis 缓存用户对话历史（键：用户 ID + 智能体 ID，过期时间 24 小时），每次请求携带上下文 ID，确保对话连贯性。 2. 升级体验功能 - 前端： - 界面优化：移除标题行、表格等冗余元素，仅保留清爽对话框；顶部保留 “设置” 图标（打开参数调节弹窗），删除 “铃铛” 图标；右上角人物图标点击弹出 “角色设置” 对话框（输入性格特质），确定时提示 “重设角色将清空 AI 记忆”； - 智能体管理：左侧侧边栏展示智能体列表（如 “通用助手”“诗词达人”），点击进入对应聊天页（标题为智能体名，消息框显示开场白）；侧边栏 “加号” 图标进入智能体新建设置页（含 token 长度、温度、设定描述输入框，新增开场白输入框）； - 格式渲染：解析 AI 返回的 markdown（如标题、列表）和 latex（公式）格式内容，转换为正常网页元素展示（如# 标题转为<h1>，E=mc 2转为公式图片）； - 移动适配：移动端左上图标点击显示侧边栏，首页上方 “加号” 图标跳转至智能体新建设置页。 - 后端： - 智能体数据服务：用 MySQL 存储智能体信息（表结构：智能体 ID、名称、开场白、默认长度、默认温度、设定描述、创建时间），提供 CRUD 接口（前端新建 / 编辑智能体时调用）； - 角色重设服务：接收 “角色重设” 请求，删除对应 Redis 上下文缓存，返回 “记忆已清空” 提示； - 语音识别集成：集成第三方语音识别 API（如百度 AI），接收前端上传的语音文件，转文字后返回给前端作为输入内容； - 格式解析服务：对 AI 返回的 markdown/latex 内容进行解析，将 markdown 转为 HTML 标签，latex 转为 SVG 公式图片，返回给前端直接渲染。 三、开发过程 1. 基础框架搭建 - 前端： - 布局设计：左侧固定侧边栏（智能体列表 + 加号按钮），右侧主区域（顶部欢迎语 + 聊天记录区 + 底部输入框 / 参数条）； - 组件开发：开发输入框组件（支持文本 / 语音）、流式消息组件（逐字加载）、进度条组件（绑定长度 / 温度参数）、对话框组件（角色设置 / 智能体新建）； - 接口封装：封装 API 请求函数（对话请求、上下文获取、智能体列表），处理流式响应（用 EventSource 监听后端 chunk 推送）。 - 后端： - 服务架构：采用 “API 网关 + 微服务” 架构，拆分 API 网关、AI 集成、上下文、智能体 4 个服务，用 Spring Boot/Spring Cloud 开发； - 数据库设计：创建 user 表（用户 ID、账号、密码）、agent 表（智能体信息）、dialogue 表（对话历史备份，可选），Redis 设计上下文缓存键（user_agent:{userID}_{agentID}）； - 中间件配置：配置 Redis（缓存上下文）、RabbitMQ（异步处理非实时任务，如对话备份），确保服务高可用。 2. 核心功能开发 - 前端： - 实时对话：输入框绑定 “发送” 事件，语音输入调用浏览器 MediaRecorder 录音，上传至后端语音识别接口；接收流式响应时，动态拼接消息内容，实现 “打字机” 效果； - 参数调节：进度条绑定 onChange 事件，实时更新页面参数显示，发送请求时将 length、temperature 参数加入请求体； - 格式渲染：引入 marked.js（解析 markdown）和 katex.js（解析 latex），在消息展示前处理内容，转换为可渲染的 DOM 元素。 - 后端： - AI 模型调用：编写 AI API 客户端（如 OpenAI Client），设置 stream=true，接收模型返回的 Flux<ChatCompletionChunk>，通过 SSE（Server-Sent Events）推送给前端； - 上下文处理：每次对话请求，从 Redis 获取历史上下文，拼接当前用户输入，传给 AI 模型；对话结束后，更新 Redis 缓存（追加新消息）； - 智能体管理：提供 agent/list（获取列表）、agent/create（新建）、agent/update（编辑）接口，处理前端参数提交，校验后存入 MySQL。 3. 交互与优化 - 前端： - 交互反馈：角色重设时弹出确认提示，参数超出范围（如长度 > 500）时显示红色提示文字；智能体切换时，清空当前聊天记录，加载对应开场白； - 性能优化：对话记录过多时（>50 条），实现 “滚动加载”（只渲染可视区域消息）；减少不必要的 API 请求（如参数未变时不重复调用）。 - 后端： - 性能优化：AI 请求超时设置（10 秒），超时返回 “请求超时，请重试”；Redis 缓存设置合理过期时间（24 小时），避免内存占用过多； - 异常处理：捕获 AI 模型调用异常（如 API 密钥失效），返回友好提示；接口返回统一格式（{code:200, msg:\"success\", data:{}}），便于前端统一处理。 4. 测试与发布 - 前端： - 功能测试：测试文本 / 语音输入、流式响应、参数调节、格式渲染（markdown 列表、latex 公式）、智能体切换； - 兼容性测试：测试 PC 端（Chrome/Firefox）、移动端（微信浏览器 / 手机 Chrome）适配效果，确保布局正常。 - 后端： - 接口测试：用 Postman 测试所有 API（参数校验、频率限制、异常场景）； - 性能测试：用 JMeter 模拟 100 用户并发请求，测试响应时间（目标 < 1 秒）、流式输出稳定性； - 发布部署：前端打包为静态资源，部署至 Nginx；后端服务容器化（Docker），部署至 K8s 集群，配置 SSL 证书确保 HTTPS 访问。",
    "task in English": "Requirement Description: Develop an intelligent AI Q&A assistant webpage to address users' daily Q&A, creation, and entertainment needs. Users can obtain personalized AI dialogue experiences (Q&A, creation, interaction) by selecting different agents (e.g., general assistant, IELTS coach) or customizing parameters (answer length: 0-500 words, temperature). The webpage needs to provide a fresh interface, real-time streaming chat, parameter adjustment, and format rendering functions; the backend should build an AI model integration, data storage, and API service system to ensure convenient operation, smooth interaction, and stable data. Detailed Functions: 1. Basic Functions - Frontend: Identity Positioning and Guidance: Each agent displays an exclusive identity (e.g., \"General Q&A Assistant\"), and 3 common question guides (e.g., \"How to adjust answer length?\") are wrapped in a light gray rounded box at the bottom left; Real-time Dialogue: The bottom input box supports text input (prompt: \"Enter a question or hold to speak...\") and voice input (hold to speak and convert to text); click send to call the backend API, display AI responses in a streaming manner (word-by-word loading effect), and retain dialogue context (show historical messages); Parameter Adjustment: Add \"Answer Length\" (0-500 words, with \"0\" and \"500\" marked at both ends of the slider) and \"Answer Temperature\" sliders in the dialog box; dragging updates the parameter value in real time (e.g., \"Answer Length: 200 words\"), and parameters are passed to the backend with the request. - Backend: API Gateway Service: Receive frontend requests, perform parameter verification (e.g., length/temperature range) and request rate limiting (max 30 requests per user per minute), and forward to corresponding business services; AI Model Integration Service: Configure third-party AI model APIs (e.g., GPT, ERNIE), set stream=true to enable streaming output, process chunk data returned by the model, and push to the frontend; Context Management Service: Use Redis to cache user dialogue history (key: userID_agentID, expiration time: 24 hours); each request carries a context ID to ensure dialogue coherence. 2. Enhanced Experience Functions - Frontend: Interface Optimization: Remove redundant elements such as title lines and tables, retaining only a clean dialog box; keep the \"Settings\" icon at the top (opens the parameter adjustment popup) and delete the \"Bell\" icon; clicking the user icon in the top right opens the \"Role Setting\" dialog box (input personality traits), with a prompt \"Resetting the role will clear AI memory\" when confirming; Agent Management: The left sidebar displays an agent list (e.g., \"General Assistant\", \"Poetry Expert\"); clicking enters the corresponding chat page (title: agent name, dialog box shows opening remarks); the \"Plus\" icon in the sidebar enters the agent creation settings page (with token length, temperature, description input boxes, and an additional opening remarks input box); Format Rendering: Parse markdown (e.g., headings, lists) and LaTeX (formulas) content returned by AI, converting to normal webpage elements for display (e.g., # Heading to <h1>, E=mc 2to formula image); Mobile Adaptation: Clicking the top-left icon on mobile devices displays the sidebar, and the \"Plus\" icon on the homepage jumps to the agent creation settings page. - Backend: Agent Data Service: Use MySQL to store agent information (table structure: agentID, name, opening remarks, default length, default temperature, description, creation time), and provide CRUD APIs (called when the frontend creates/edits agents); Role Reset Service: Receive \"role reset\" requests, delete the corresponding Redis context cache, and return the prompt \"Memory cleared\"; Speech Recognition Integration: Integrate third-party speech recognition APIs (e.g., Baidu AI), receive voice files uploaded by the frontend, convert to text, and return to the frontend as input content; Format Parsing Service: Parse markdown/LaTeX content returned by AI, convert markdown to HTML tags and LaTeX to SVG formula images, and return to the frontend for direct rendering. Development Process: 1. Basic Framework Construction - Frontend: Layout Design: Fixed left sidebar (agent list + plus button), right main area (top welcome message + chat history area + bottom input box/parameter sliders); Component Development: Develop input box components (support text/voice), streaming message components (word-by-word loading), slider components (bind length/temperature parameters), and dialog components (role setting/agent creation); Interface Encapsulation: Encapsulate API request functions (dialogue request, context acquisition, agent list), and handle streaming responses (listen for backend chunk pushes using EventSource). - Backend: Service Architecture: Adopt \"API Gateway + Microservices\" architecture, split into 4 services (API Gateway, AI Integration, Context, Agent), developed with Spring Boot/Spring Cloud; Database Design: Create user table (userID, account, password), agent table (agent information), dialogue table (dialogue history backup, optional); design Redis context cache key (user_agent:{userID}_{agentID}); Middleware Configuration: Configure Redis (cache context) and RabbitMQ (asynchronously process non-real-time tasks, e.g., dialogue backup) to ensure high service availability. 2. Core Function Development - Frontend: Real-time Dialogue: Bind the \"send\" event to the input box; voice input calls the browser MediaRecorder to record, upload to the backend speech recognition API; when receiving streaming responses, dynamically splice message content to achieve a \"typewriter\" effect; Parameter Adjustment: Bind the onChange event to the sliders, update parameter display in real time, and add length/temperature parameters to the request body when sending requests; Format Rendering: Introduce marked.js (parse markdown) and katex.js (parse LaTeX), process content before message display, and convert to renderable DOM elements. - Backend: AI Model Calling: Write an AI API client (e.g., OpenAI Client), set stream=true, receive Flux<ChatCompletionChunk> returned by the model, and push to the frontend via SSE (Server-Sent Events); Context Processing: For each dialogue request, obtain historical context from Redis, splice with current user input, and pass to the AI model; after the dialogue ends, update the Redis cache (append new messages); Agent Management: Provide agent/list (get list), agent/create (create), agent/update (edit) APIs, process frontend parameter submissions, and store in MySQL after verification. 3. Interaction and Optimization - Frontend: Interaction Feedback: Pop up a confirmation prompt when resetting the role; display red prompt text when parameters exceed the range (e.g., length > 500); clear current chat records and load corresponding opening remarks when switching agents; Performance Optimization: Implement \"infinite scroll\" for excessive dialogue records (>50 items, only render visible messages); reduce unnecessary API requests (e.g., no repeated calls when parameters remain unchanged). - Backend: Performance Optimization: Set AI request timeout (10 seconds), return \"Request timed out, please try again\" on timeout; set a reasonable Redis cache expiration time (24 hours) to avoid excessive memory usage; Error Handling: Capture AI model calling errors (e.g., invalid API key), return user-friendly prompts; return a unified interface format ({code:200, msg:\"success\", data:{}}) for unified frontend processing. 4. Testing and Deployment - Frontend: Functional Testing: Test text/voice input, streaming responses, parameter adjustment, format rendering (markdown lists, LaTeX formulas), and agent switching; Compatibility Testing: Test adaptation effects on PC (Chrome/Firefox) and mobile (WeChat Browser/Mobile Chrome) to ensure normal layout. - Backend: Interface Testing: Test all APIs with Postman (parameter verification, rate limiting, exception scenarios); Performance Testing: Simulate 100 concurrent user requests with JMeter, test response time (target: <1 second) and streaming output stability; Deployment: Package the frontend into static resources and deploy to Nginx; containerize backend services (Docker), deploy to a K8s cluster, and configure SSL certificates to ensure HTTPS access."
}